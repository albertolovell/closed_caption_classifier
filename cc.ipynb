{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import datetime\n",
    "import string\n",
    "from string import digits\n",
    "import collections\n",
    "import scipy.stats as scs\n",
    "import cc_pipeline as P\n",
    "import time\n",
    "import random\n",
    "\n",
    "#sentiment and language\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import vaderSentiment\n",
    "from langdetect import detect\n",
    "from gensim.models import Word2Vec\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import knee_locator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plotting\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_text(series):\n",
    "    \n",
    "    #use pd.series instead moving forward\n",
    "    text = []\n",
    "    for link in series:\n",
    "        \n",
    "        try:\n",
    "            req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            webpage = urlopen(req).read()\n",
    "            webpage = webpage.decode('utf-8')\n",
    "            text.append(webpage)\n",
    "        except SocketError as e:\n",
    "            if e.errno != errno.ECONNRESET:\n",
    "                raise # Not error we are looking for\n",
    "            pass # Handle error here.\n",
    "        \n",
    "    return text\n",
    "\n",
    "def scrape_from_url(cycles):\n",
    "    \n",
    "    for i in range(cycles):\n",
    "        \n",
    "        time.sleep(900)\n",
    "        \n",
    "        from_df = pd.read_csv('data/cc_recent.csv', encoding='utf-8')\n",
    "        temp_df = from_df.head(1000)\n",
    "\n",
    "        #scrape text then concat\n",
    "        text_url_series = pd.Series(temp_df['url'])\n",
    "        extracts = scrape_text(text_url_series)\n",
    "        extracts = pd.Series(extracts)\n",
    "        temp_df['text'] = extracts\n",
    "\n",
    "        head_df = pd.read_csv('data/cc_head_text.csv', encoding='utf-8')\n",
    "        head_df = pd.concat([head_df, temp_df])\n",
    "        head_df.to_csv('data/cc_head_text.csv', encoding='utf-8', index=False)\n",
    "\n",
    "        from_df = from_df.drop(from_df.index[0:1000])\n",
    "        from_df.to_csv('data/cc_recent.csv', encoding='utf-8', index=False)\n",
    "\n",
    "        head_df=None\n",
    "        temp_df=None\n",
    "        from_df=None\n",
    "        text_url_series=None\n",
    "        extracts=None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape_from_url(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean text\n",
    "\n",
    "def get_show_text(show_raw):\n",
    "    \n",
    "    '''returns show text without timestamps'''\n",
    "    \n",
    "    return \"\\n\".join( [\"\\n\".join( x.split(\"\\n\")[2:] ) for x in show_raw.split(\"\\n\\n\")] )\n",
    "\n",
    "\n",
    "\n",
    "def clean_all_text(text_list):\n",
    "    \n",
    "    '''cleans all text and creates new column in dataframe'''\n",
    "    \n",
    "    doc_list = []\n",
    "    for word in text_list:\n",
    "\n",
    "        for word in text_list:\n",
    "            doc_list.append(get_show_text(word))\n",
    "    return doc_list\n",
    "\n",
    "\n",
    "def clean_text(doc):\n",
    "    '''cleans and lemmatizes a string by removing punc, characters, digits, and len(words) < 3'''\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    punct = ('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~♪¿’')\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = []\n",
    "    \n",
    "    doc = doc.split('\\n')\n",
    "    doc = ' '.join(doc)\n",
    "    doc = doc.split('-')\n",
    "    doc = ' '.join(doc)\n",
    "    doc = doc.split('...')\n",
    "    doc = ' '.join(doc)\n",
    "    doc = word_tokenize(doc)\n",
    "\n",
    "    a = [char for char in doc if char not in punct]\n",
    "    b = [w for w in a if w not in stop_words] \n",
    "    c = [w for w in b if len(w) > 3]\n",
    "    d = [x for x in c if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n",
    "\n",
    "    e = ' '.join(d)\n",
    "    f = e.lower()\n",
    "    g = f.translate(remove_digits)\n",
    "    cleaned = str(g)\n",
    "    doc = word_tokenize(cleaned)\n",
    "    \n",
    "    for val in doc:\n",
    "        doc_temp = wordnet_lemmatizer.lemmatize(val)\n",
    "        lemmatized.append(doc_temp)\n",
    "    doc = ' '.join(lemmatized)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def clean_and_return(docs_list):\n",
    "    \n",
    "    docs = []\n",
    "    for cc in docs_list:\n",
    "        cleaned_temp = clean_text(cc)\n",
    "        docs.append(cleaned_temp)\n",
    "        \n",
    "    return docs\n",
    "\n",
    "\n",
    "def lang_detect(doc_series):\n",
    "    \n",
    "    lang = []\n",
    "    for x in doc_series:\n",
    "        eng = 'en'\n",
    "        span = 'es'\n",
    "\n",
    "        try:\n",
    "            if detect(x) == eng:\n",
    "                lang.append(eng)\n",
    "            else:\n",
    "                lang.append(span)\n",
    "        except:\n",
    "            lang.append(None)\n",
    "            \n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv('data/cc_1000_text.csv', encoding='utf-8')\n",
    "temp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp = temp_df['text'].values\n",
    "temp = temp.tolist()\n",
    "type(temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = clean_all_text(temp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_list = clean_and_return(docs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['cleaned'] = cleaned_list\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_series = pd.Series(temp_df['cleaned'].values)\n",
    "language = lang_detect(doc_series)\n",
    "temp_df['language'] = language\n",
    "english = temp_df[temp_df['language'] == 'en']\n",
    "spanish = temp_df[temp_df['language'] == 'es']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english.to_csv('data/testenglish.csv', encoding='utf-8', index=False)\n",
    "# spanish.to_csv('data/testspanish.csv', encoding='utf-8', index=False)\n",
    "english.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
