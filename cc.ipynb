{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import datetime\n",
    "import string\n",
    "from string import digits\n",
    "import collections\n",
    "import scipy.stats as scs\n",
    "import cc_pipeline as P\n",
    "import time\n",
    "import random\n",
    "\n",
    "#sentiment and language\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import vaderSentiment\n",
    "from langdetect import detect\n",
    "from gensim.models import Word2Vec\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import knee_locator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plotting\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_text(series):\n",
    "    \n",
    "    #use pd.series instead moving forward\n",
    "    text = []\n",
    "    for link in series:\n",
    "        \n",
    "        try:\n",
    "            req = Request(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            webpage = urlopen(req).read()\n",
    "            webpage = webpage.decode('utf-8')\n",
    "            text.append(webpage)\n",
    "        except SocketError as e:\n",
    "            if e.errno != errno.ECONNRESET:\n",
    "                raise # Not error we are looking for\n",
    "            pass # Handle error here.\n",
    "        \n",
    "    return text\n",
    "\n",
    "def scrape_from_url(cycles):\n",
    "    \n",
    "    for i in range(cycles):\n",
    "        \n",
    "        time.sleep(900)\n",
    "        \n",
    "        from_df = pd.read_csv('data/cc_recent.csv', encoding='utf-8')\n",
    "        temp_df = from_df.head(1000)\n",
    "\n",
    "        #scrape text then concat\n",
    "        text_url_series = pd.Series(temp_df['url'])\n",
    "        extracts = scrape_text(text_url_series)\n",
    "        extracts = pd.Series(extracts)\n",
    "        temp_df['text'] = extracts\n",
    "\n",
    "        head_df = pd.read_csv('data/cc_head_text.csv', encoding='utf-8')\n",
    "        head_df = pd.concat([head_df, temp_df])\n",
    "        head_df.to_csv('data/cc_head_text.csv', encoding='utf-8', index=False)\n",
    "\n",
    "        from_df = from_df.drop(from_df.index[0:1000])\n",
    "        from_df.to_csv('data/cc_recent.csv', encoding='utf-8', index=False)\n",
    "\n",
    "        head_df=None\n",
    "        temp_df=None\n",
    "        from_df=None\n",
    "        text_url_series=None\n",
    "        extracts=None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape_from_url(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean text\n",
    "\n",
    "def get_show_text(show_raw):\n",
    "    \n",
    "    '''returns show text without timestamps'''\n",
    "    \n",
    "    return \"\\n\".join( [\"\\n\".join( x.split(\"\\n\")[2:] ) for x in show_raw.split(\"\\n\\n\")] )\n",
    "\n",
    "\n",
    "\n",
    "def clean_all_text(text_list):\n",
    "    \n",
    "    '''cleans all text and creates new column in dataframe'''\n",
    "    \n",
    "    doc_l\n",
    "    for word in text_list:\n",
    "        doc_list.append(get_show_text(word))\n",
    "    return doc_list\n",
    "\n",
    "\n",
    "def clean_text(doc):\n",
    "    '''cleans and lemmatizes a string by removing punc, characters, digits, and len(words) < 3'''\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    punct = ('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~♪¿’')\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = []\n",
    "    \n",
    "    doc = doc.split('\\n')\n",
    "    doc = ' '.join(doc)\n",
    "    doc = doc.split('-')\n",
    "    doc = ' '.join(doc)\n",
    "    doc = doc.split('...')\n",
    "    doc = ' '.join(doc)\n",
    "    doc = word_tokenize(doc)\n",
    "\n",
    "    a = [char for char in doc if char not in punct]\n",
    "    b = [w for w in a if w not in stop_words] \n",
    "    c = [w for w in b if len(w) > 3]\n",
    "    d = [x for x in c if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n",
    "\n",
    "    e = ' '.join(d)\n",
    "    f = e.lower()\n",
    "    g = f.translate(remove_digits)\n",
    "    cleaned = str(g)\n",
    "    doc = word_tokenize(cleaned)\n",
    "    \n",
    "    for val in doc:\n",
    "        doc_temp = wordnet_lemmatizer.lemmatize(val)\n",
    "        lemmatized.append(doc_temp)\n",
    "    doc = ' '.join(lemmatized)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def clean_and_return(docs_list):\n",
    "    \n",
    "    docs = []\n",
    "    for cc in docs_list:\n",
    "        cleaned_temp = clean_text(cc)\n",
    "        docs.append(cleaned_temp)\n",
    "        \n",
    "    return docs\n",
    "\n",
    "\n",
    "def lang_detect(doc_series):\n",
    "    \n",
    "    lang = []\n",
    "    for x in doc_series:\n",
    "        eng = 'en'\n",
    "        span = 'es'\n",
    "\n",
    "        try:\n",
    "            if detect(x) == eng:\n",
    "                lang.append(eng)\n",
    "            else:\n",
    "                lang.append(span)\n",
    "        except:\n",
    "            lang.append(None)\n",
    "            \n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>created_at</th>\n",
       "      <th>duration</th>\n",
       "      <th>lang</th>\n",
       "      <th>machine_id</th>\n",
       "      <th>ts</th>\n",
       "      <th>tui</th>\n",
       "      <th>tv</th>\n",
       "      <th>url</th>\n",
       "      <th>zip_url</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'$oid': '5bb2b5082c1ba60007fc013d'}</td>\n",
       "      <td>WBNX_HD_CW</td>\n",
       "      <td>{'$date': '2018-10-02T00:00:08.982Z'}</td>\n",
       "      <td>900000</td>\n",
       "      <td>dflt</td>\n",
       "      <td>ENSWERCCR7_8_3</td>\n",
       "      <td>{'$numberLong': '1538437500000'}</td>\n",
       "      <td>251536767</td>\n",
       "      <td>{'$date': '2018-10-01T23:45:00.000Z'}</td>\n",
       "      <td>https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...</td>\n",
       "      <td>https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...</td>\n",
       "      <td>2018-10-02 00:00:08.982</td>\n",
       "      <td>1538437621406\\n1835\\n[ KNOCK ON DOOR ]\\n\\n1538...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'$oid': '5bb2b5106f40ed000728b879'}</td>\n",
       "      <td>KZJO_MY_NETWORK_SEATTLE</td>\n",
       "      <td>{'$date': '2018-10-02T00:00:16.702Z'}</td>\n",
       "      <td>900000</td>\n",
       "      <td>dflt</td>\n",
       "      <td>ENSWERCCR7_48_2</td>\n",
       "      <td>{'$numberLong': '1538437500000'}</td>\n",
       "      <td>251533435</td>\n",
       "      <td>{'$date': '2018-10-01T23:45:00.000Z'}</td>\n",
       "      <td>https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...</td>\n",
       "      <td>https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...</td>\n",
       "      <td>2018-10-02 00:00:16.702</td>\n",
       "      <td>1538437521225\\n1435\\nSeen it, covered it.\\n\\n1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'$oid': '5bb2b51170969a000bc9a14e'}</td>\n",
       "      <td>WVUA_THIS_TV_BIRMINGHAM</td>\n",
       "      <td>{'$date': '2018-10-02T00:00:17.995Z'}</td>\n",
       "      <td>900000</td>\n",
       "      <td>dflt</td>\n",
       "      <td>ENSWERCCR7_65_236</td>\n",
       "      <td>{'$numberLong': '1538435700000'}</td>\n",
       "      <td>251541606</td>\n",
       "      <td>{'$date': '2018-10-01T23:15:00.000Z'}</td>\n",
       "      <td>https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...</td>\n",
       "      <td>https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...</td>\n",
       "      <td>2018-10-02 00:00:17.995</td>\n",
       "      <td>1538435707355\\n4571\\n\u0001\\n\u0001\\n\\n1538435711926\\n23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'$oid': '5bb2b51187701400079ec41d'}</td>\n",
       "      <td>TV_LAND</td>\n",
       "      <td>{'$date': '2018-10-02T00:00:17.925Z'}</td>\n",
       "      <td>900000</td>\n",
       "      <td>dflt</td>\n",
       "      <td>ENSWERCCR4_22</td>\n",
       "      <td>{'$numberLong': '1538437500000'}</td>\n",
       "      <td>251536903</td>\n",
       "      <td>{'$date': '2018-10-01T23:45:00.000Z'}</td>\n",
       "      <td>https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...</td>\n",
       "      <td>https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...</td>\n",
       "      <td>2018-10-02 00:00:17.925</td>\n",
       "      <td>1538437500869\\n2653\\nWe’ll talk later.\\n\\n1538...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'$oid': '5bb2b511b18af90007b4e445'}</td>\n",
       "      <td>WPCH_INDEPENDENT_ATLANTA</td>\n",
       "      <td>{'$date': '2018-10-02T00:00:17.074Z'}</td>\n",
       "      <td>900000</td>\n",
       "      <td>dflt</td>\n",
       "      <td>ENSWERCCR7_7_245</td>\n",
       "      <td>{'$numberLong': '1538437500000'}</td>\n",
       "      <td>251534109</td>\n",
       "      <td>{'$date': '2018-10-01T23:45:00.000Z'}</td>\n",
       "      <td>https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...</td>\n",
       "      <td>https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...</td>\n",
       "      <td>2018-10-02 00:00:17.074</td>\n",
       "      <td>1538437500144\\n1201\\nIt hurts.\\n\\n153843750218...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    _id                   channel  \\\n",
       "0  {'$oid': '5bb2b5082c1ba60007fc013d'}                WBNX_HD_CW   \n",
       "1  {'$oid': '5bb2b5106f40ed000728b879'}   KZJO_MY_NETWORK_SEATTLE   \n",
       "2  {'$oid': '5bb2b51170969a000bc9a14e'}   WVUA_THIS_TV_BIRMINGHAM   \n",
       "3  {'$oid': '5bb2b51187701400079ec41d'}                   TV_LAND   \n",
       "4  {'$oid': '5bb2b511b18af90007b4e445'}  WPCH_INDEPENDENT_ATLANTA   \n",
       "\n",
       "                              created_at  duration  lang         machine_id  \\\n",
       "0  {'$date': '2018-10-02T00:00:08.982Z'}    900000  dflt     ENSWERCCR7_8_3   \n",
       "1  {'$date': '2018-10-02T00:00:16.702Z'}    900000  dflt    ENSWERCCR7_48_2   \n",
       "2  {'$date': '2018-10-02T00:00:17.995Z'}    900000  dflt  ENSWERCCR7_65_236   \n",
       "3  {'$date': '2018-10-02T00:00:17.925Z'}    900000  dflt      ENSWERCCR4_22   \n",
       "4  {'$date': '2018-10-02T00:00:17.074Z'}    900000  dflt   ENSWERCCR7_7_245   \n",
       "\n",
       "                                 ts        tui  \\\n",
       "0  {'$numberLong': '1538437500000'}  251536767   \n",
       "1  {'$numberLong': '1538437500000'}  251533435   \n",
       "2  {'$numberLong': '1538435700000'}  251541606   \n",
       "3  {'$numberLong': '1538437500000'}  251536903   \n",
       "4  {'$numberLong': '1538437500000'}  251534109   \n",
       "\n",
       "                                      tv  \\\n",
       "0  {'$date': '2018-10-01T23:45:00.000Z'}   \n",
       "1  {'$date': '2018-10-01T23:45:00.000Z'}   \n",
       "2  {'$date': '2018-10-01T23:15:00.000Z'}   \n",
       "3  {'$date': '2018-10-01T23:45:00.000Z'}   \n",
       "4  {'$date': '2018-10-01T23:45:00.000Z'}   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...   \n",
       "1  https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...   \n",
       "2  https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...   \n",
       "3  https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...   \n",
       "4  https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...   \n",
       "\n",
       "                                             zip_url                     date  \\\n",
       "0  https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...  2018-10-02 00:00:08.982   \n",
       "1  https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...  2018-10-02 00:00:16.702   \n",
       "2  https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...  2018-10-02 00:00:17.995   \n",
       "3  https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...  2018-10-02 00:00:17.925   \n",
       "4  https://s3.amazonaws.com/adm-mpeg-dash/gnvideo...  2018-10-02 00:00:17.074   \n",
       "\n",
       "                                                text  \n",
       "0  1538437621406\\n1835\\n[ KNOCK ON DOOR ]\\n\\n1538...  \n",
       "1  1538437521225\\n1435\\nSeen it, covered it.\\n\\n1...  \n",
       "2  1538435707355\\n4571\\n\u0001\\n\u0001\\n\\n1538435711926\\n23...  \n",
       "3  1538437500869\\n2653\\nWe’ll talk later.\\n\\n1538...  \n",
       "4  1538437500144\\n1201\\nIt hurts.\\n\\n153843750218...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.read_csv('data/cc_1000_text.csv', encoding='utf-8')\n",
    "temp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "temp = temp_df['text'].values\n",
    "temp = temp.tolist()\n",
    "type(temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-733f62764231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocs_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_all_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f0f04a1f3018>\u001b[0m in \u001b[0;36mclean_all_text\u001b[0;34m(text_list)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdoc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_show_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_list' is not defined"
     ]
    }
   ],
   "source": [
    "docs_list = clean_all_text(temp)\n",
    "type(docs_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_list = clean_and_return(docs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['cleaned'] = cleaned_list\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_series = pd.Series(temp_df['cleaned'].values)\n",
    "language = lang_detect(doc_series)\n",
    "temp_df['language'] = language\n",
    "english = temp_df[temp_df['language'] == 'en']\n",
    "spanish = temp_df[temp_df['language'] == 'es']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english.to_csv('data/testenglish.csv', encoding='utf-8', index=False)\n",
    "# spanish.to_csv('data/testspanish.csv', encoding='utf-8', index=False)\n",
    "english.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
