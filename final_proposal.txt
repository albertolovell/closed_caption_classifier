Final Proposal: Closed Caption Unsupervised Learning


Closed caption data allows for an in-depth exploration of the sentiment of a show or broadcast network. Using unstructured closed caption transcripts and metadata such as channel, timestamp, anonymized TV show id, and duration, I plan to explore underlying correlations between the vocabulary and word distribution of a show or broadcast station and the sentiment of those shows using unsupervised learning, clustering, and nlp. Specifically, I will find the sentiment of each show/station (positive, negative, or neutral) based on keywords or n-gram phrases. Moving forward I would like to find correlation between tv-shows/broadcast stations and top 100 brands mentioned during airing to determine if there is any effect on sentiment.

Questions:
What is the sentiment of specific shows or broadcast station?
How does presence of top 100 brands affect sentiment?

Since the raw data is not well formatted, I plan to create a pipeline to standardize, clean, and transform the entire set. I will then apply modeling techniques to find insteresting insights or relationships inherent in the data.

I anticipate possible issues with the size and formatting of the dataset. Each unique ID contains metadata as well as a url to closed-caption information formatted as a .cc file. I anticipate potential issues during file extraction/conversion and will need to explore the most efficient method to import raw text data for analysis. Also, since this is a large dataset, I may need to implement a cloud-based method during modeling which will take time to set up the environment.

Initially I will tune and evaluate a simple unsupervised learning model to cluster content using various similarity measures. I then plan to move forward by using dimensionality reduction techniques to further discriminate between clusters and create a descriptive interactive visualization.

If possible, I will then use timestamp information to explore any underlying relationships between trending topics and show/channel content, as well as characteristics of word distribution during specific time windows. I would also like to use this data to create a classifier or recommender and explore complex word indentifiers (CWI) for future data as it becomes available.

This project will rely heavily on python's scikitlearn and NLTK libraries. I plan to use visualization libraries such as bokeh, plot.ly, seaborn etc. and any additional proprietary libraries will be cited. I will also acknowledge Gracenote/Nielsen for providing the data in accordance with a signed Non-Disclosure Agreement.


Data ('img/cc_metadata.png', 'img/cc_raw_text.png')

Raw data provided by Gracenote/Nielsen was extracted via api and stored as a .json file with embedded urls linking to .cc closed caption files for each unique TV show ID. 

Metadata includes: channel name, timestamp, title unique id (TV show), duration (milliseconds), url (.cc caption file)
Caption files include: timestamp, duration, captions 

Metadata Example:

 u'_id': {u'$oid': u'5b6dc4b66f40ed0007a35e50'},
 u'channel': u'KCPQ_FOX_SEATTLE',
 u'created_at': {u'$date': u'2018-08-10T17:00:38.690Z'},
 u'duration': 900000,
 u'lang': u'dflt',
 u'machine_id': u'ENSWERCCR7_48_2',
 u'ts': {u'$numberLong': u'1533919500000'},
 u'tui': u'251536137',
 u'tv': {u'$date': u'2018-08-10T16:45:00.000Z'},
 u'url': u'https://s3.amazonaws.com/adm-mpeg-dash/gnvideostream/ENSWERCCR7_48_2/KCPQ_FOX_SEATTLE/ccaptions/2018/08/10/d871a12943c40894b2ef9a244dd5704a.cc',
 u'zip_url': u'https://s3.amazonaws.com/adm-mpeg-dash/gnvideostream/ENSWERCCR7_48_2/KCPQ_FOX_SEATTLE/ccaptions/2018/08/10/d871a12943c40894b2ef9a244dd5704a.cc.gz'
 
Raw Text Example:

1533918614077
634
RENTON CURVES SOUTHBOUND 405
HITTING THE BRAKES CLOSER

1533918614711
634
HITTING THE BRAKES CLOSER
TOWARDS THE SNOHOMISH KING


Schedule:
11/01-11/05 (Mon): data  ETL: clean/transform data and design data modeling to which you think make most sense. Then load it either to sql, elasticsearch, or any data warehouse tool you like.

11/06 -11/09 (Fri): choose your own topic: you have raw data now, how you can tell a story to everyone from the data?

11/12 -11/16 (Fri): implementation and visualization: share the result and methodology you use.

11/19- 11/21(Wed): review/discussion


Brandon:
What's the most angry word?
What's the saddest time of day on TV? Happiest day of the year on TV?
Which celebrity boosts the happiness of shows the most?


Moses:
How are you going to quantify "sentiment"? A third party package like VADER?

I like the idea of doing topic analysis by channel / show / time of day... but that's gonna take a lot of human validation (that is, you'll have to read the words that are most common for each cluster / topic, and identify if it makes sense). How much time will you spend on this part of the project? The first thing you should do is make a plan: what will you explore first, what will you explore second, how long will each thing take. Make a loose outline of your schedule / goals for the next two weeks. This'll help keep you from going down the rabbit hole.


