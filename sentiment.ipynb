{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import datetime\n",
    "import string\n",
    "from string import digits\n",
    "import collections\n",
    "import scipy.stats as scs\n",
    "import cc_pipeline as P\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "#sentiment and language\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import vaderSentiment\n",
    "from langdetect import detect\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from spacy import displacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import chi2\n",
    "import knee_locator\n",
    "\n",
    "#plotting\n",
    "from bokeh.plotting import figure, show, output_file, output_notebook, ColumnDataSource\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "import umap\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inline_text(show_raw):\n",
    "    \n",
    "    '''returns show text without timestamps'''\n",
    "    \n",
    "    temp = \" \".join( [\"\\n\".join( x.split(\"\\n\")[2:] ) for x in show_raw.split(\"\\n\\n\")] )\n",
    "    temp = temp.split('\\n')\n",
    "    temp = \" \".join(temp)\n",
    "    return temp\n",
    "\n",
    "def clean_for_spacy(text_list):\n",
    "    \n",
    "    '''cleans all text and creates new column in dataframe'''\n",
    "    \n",
    "    doc_list = []\n",
    "    for doc in text_list:\n",
    "        doc_list.append(inline_text(doc))\n",
    "    return doc_list\n",
    "\n",
    "def clean_for_spacy_lower(text_list):\n",
    "    \n",
    "    '''cleans all text and creates new column in dataframe'''\n",
    "    \n",
    "    doc_list = []\n",
    "    for doc in text_list:\n",
    "        doc_list.append(inline_text(doc).lower())\n",
    "    return doc_list\n",
    "\n",
    "def sent_for_spacy(text_list):\n",
    "    \n",
    "    '''cleans all text and creates new column in dataframe'''\n",
    "    \n",
    "    doc_list = []\n",
    "    for doc in text_list:\n",
    "        cleaned = inline_text(doc)\n",
    "        tok = sent_tokenize(cleaned)\n",
    "        doc_list.append(tok)\n",
    "    return doc_list\n",
    "\n",
    "def lang_detect(doc_series):\n",
    "    \n",
    "    lang = []\n",
    "    for x in doc_series:\n",
    "        eng = 'en'\n",
    "        span = 'es'\n",
    "\n",
    "        try:\n",
    "            if detect(x) == eng:\n",
    "                lang.append(eng)\n",
    "            else:\n",
    "                lang.append(span)\n",
    "        except:\n",
    "            lang.append(None)\n",
    "            \n",
    "    return lang\n",
    "\n",
    "def get_orgs(chunks):\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    orgs = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        document = nlp(chunk)\n",
    "        labels = set([w.label_ for w in document.ents]) \n",
    "\n",
    "        for label in labels: \n",
    "\n",
    "            temp_entities = [e for e in document.ents if label==e.label_] \n",
    "            temp_entities = list(set(temp_entities)) \n",
    "\n",
    "            if label == 'ORG':\n",
    "                orgs.append(str(temp_entities))\n",
    "                \n",
    "    orgs = \" \".join(orgs)\n",
    "    return orgs\n",
    "\n",
    "def clean_text(doc):\n",
    "    '''cleans and lemmatizes a string by removing punc, characters, digits, and len(words) < 3'''\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    punct = ('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~♪¿’')\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = []\n",
    "    \n",
    "    doc = doc.split('\\n')\n",
    "    doc = ' '.join(doc)\n",
    "    doc = doc.split('-')\n",
    "    doc = ' '.join(doc)\n",
    "    doc = doc.split('...')\n",
    "    doc = ' '.join(doc)\n",
    "    doc = word_tokenize(doc)\n",
    "\n",
    "    a = [char for char in doc if char not in punct]\n",
    "    b = [w for w in a if w not in stop_words] \n",
    "    c = [w for w in b if len(w) > 3]\n",
    "    d = [x for x in c if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n",
    "\n",
    "    e = ' '.join(d)\n",
    "    f = e.lower()\n",
    "    g = f.translate(remove_digits)\n",
    "    cleaned = str(g)\n",
    "    doc = word_tokenize(cleaned)\n",
    "    \n",
    "    for val in doc:\n",
    "        doc_temp = wordnet_lemmatizer.lemmatize(val)\n",
    "        lemmatized.append(doc_temp)\n",
    "    doc = ' '.join(lemmatized)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def clean_and_return(docs_list):\n",
    "    \n",
    "    docs = []\n",
    "    for cc in docs_list:\n",
    "        cleaned_temp = clean_text(cc)\n",
    "        docs.append(cleaned_temp)\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = pd.read_csv('data/cc_1000_text.csv', encoding='utf=8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english only (for testing)\n",
    "\n",
    "doc_series = pd.Series(sent_df['text'].values)\n",
    "language = lang_detect(doc_series)\n",
    "sent_df['language'] = language\n",
    "english = sent_df[sent_df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brand_text = clean_and_return(lines)\n",
    "# removetable = str.maketrans('', '', \"/.\")\n",
    "# brand_text = [s.translate(removetable) for s in brand_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_text = english['text'].values\n",
    "sent_text = clean_for_spacy(temp_text)\n",
    "sent_text_lower = clean_for_spacy_lower(temp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tok = sent_tokenize(sent_text_lower[1])\n",
    "type(sent_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_text_lower[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_text = clean_and_return(sent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = \" \".join(sent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #break into chunks for spaCy\n",
    "\n",
    "# n = 100000\n",
    "# chunks = [corpus[i:i+n] for i in range(0, len(corpus), n)]\n",
    "# len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# org_list = get_orgs(chunks)\n",
    "# org_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accenture', 'adidas', 'adobe', 'agricultural bank of china', 'alibaba', 'amazon', 'american express', 'apple', 'at&t', 'baidu', 'bank of america', 'bank of china', 'bmw', 'budweiser', 'chase', 'china construction bank', 'china life', 'china mobile', 'cisco', 'citi', 'coca-cola', 'colgate', 'commonwealth bank of australia', 'costco', 'deutsche telekom', 'dhl', 'disney', 'ebay', 'exxonmobil', 'facebook', 'fedex', 'ford', 'gillette', 'google', 'gucci', 'hdfc bank', 'hermès', 'honda', 'hp', 'hsbc', 'huawei', 'ibm', 'icbc', 'ikea', 'instagram', 'intel', 'jp morgan', 'jd.com', 'kfc', \"l'oréal paris\", 'linkedin', 'louis vuitton', \"lowe's\", 'marlboro', 'mastercard', \"mcdonald's\", 'mercedes benz', 'microsoft', 'moutai', 'movistar', 'netflix', 'nike', 'oracle', 'pampers', 'paypal', 'pepsi', 'salesforce', 'samsung', 'shell', 'siemens', 'spectrum', 'starbucks', 'subway', 'tencent', 'the home depot', 'toyota', 'uber', 'us bank', 'verizon', 'visa', 'vodafone', 'walmart', 'wells fargo', 'xfinity', 'youtube', 'zara, 'tide', 'whole foods', 'secret service', 'allstate', 'twitter', 'nintendo', 'lincoln', 'medicare', 'philadelphia eagles', 'super bowl', 'aarp', 'nfl', 'miller', 'farmers', 'geico', 'santa', 'cardi b', 'enbrel', 'fbi', 'disney', 'christmas', 'navy', 'humana', 'discover', 'prudential', 'tresiba', 'north american trade agreement', 'dodge', 'at&t', 'ebay', 'trump', 'obama', 'cbs', 'bridgestone', 'hyundai', 'kia', 'mexico', 'chipotle', 'directv', 'social media', 'kanye', 'tmz', 'applebee', 'stanford', 'target', 'alexa', 'kavanaugh', 'autozone', 'centrum', 'chocolate', 'vaseline', 'chp', 'aclu', 'ford', 'army', 'the big bang theory', 'nasa', 'fox', 'kfc', 'ariana grande', 'mcdonald', 'chevy', 'walmart', 'downy', 'nyquil', 'ancestrydna', 'pillsbury', 'amazon', 'sprint', 'iphone', 'bmx', 'liberty mutual', 'target', 'universal studios', 'microsoft']\n"
     ]
    }
   ],
   "source": [
    "# with open(\"brand_list.txt\") as f:\n",
    "#     short_brands = f.read().replace('\\n', '').lower()\n",
    "# print(short_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brands = pd.read_csv('data/all_brands.csv', encoding='utf-8', header=None)\n",
    "# brand_names = brands[0].values\n",
    "# brand_names = list(brand_names)\n",
    "# lines = (lines + str(brand_names))\n",
    "# lines = lines.lower()\n",
    "\n",
    "# #conver string of list into list\n",
    "# import ast\n",
    "# x = lines\n",
    "# x = list(x.replace(\"'\", '').replace('[', '').replace(']', '').split(', '))\n",
    "# lines = x\n",
    "\n",
    "# # with open ('data/brands.pkl', 'wb') as f:\n",
    "# #     pickle.dump(lines, f)\n",
    "    \n",
    "# # with open ('data/brands.pkl', 'rb') as r:\n",
    "# #     brands = pickle.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = short_brands\n",
    "# y = list(y.replace(\"'\", '').replace('[', '').replace(']', '').replace('\\\"','').replace('-', '').split(', '))\n",
    "# short_brands = y\n",
    "# with open ('data/short_brands_lower.pkl', 'wb') as f:\n",
    "#     pickle.dump(short_brands, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sentiment if brand present in text\n",
    "\n",
    "def get_sentiment_score(doc, brands):\n",
    "    \n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    temp_doc = doc.split()\n",
    "    temp = (brand for brand in brands if brand in temp_doc)\n",
    "    all_brands = []\n",
    "    scores = []\n",
    "    if any(temp):\n",
    "        for brand in temp:\n",
    "            all_brands.append(brand)\n",
    "            score = list(dict.items(analyser.polarity_scores(doc)))\n",
    "            scores.append(score)\n",
    "    return (all_brands, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['santa', 'christmas'],\n",
       " [[('neg', 0.048), ('neu', 0.622), ('pos', 0.331), ('compound', 0.9998)],\n",
       "  [('neg', 0.048), ('neu', 0.622), ('pos', 0.331), ('compound', 0.9998)]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = get_sentiment_score(score_text[1], short_brands)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['netflix', 'santa', 'trump', 'obama', 'chocolate', 'army'],\n",
       " [[('neg', 0.085), ('neu', 0.673), ('pos', 0.241), ('compound', 0.9999)],\n",
       "  [('neg', 0.085), ('neu', 0.673), ('pos', 0.241), ('compound', 0.9999)],\n",
       "  [('neg', 0.085), ('neu', 0.673), ('pos', 0.241), ('compound', 0.9999)],\n",
       "  [('neg', 0.085), ('neu', 0.673), ('pos', 0.241), ('compound', 0.9999)],\n",
       "  [('neg', 0.085), ('neu', 0.673), ('pos', 0.241), ('compound', 0.9999)],\n",
       "  [('neg', 0.085), ('neu', 0.673), ('pos', 0.241), ('compound', 0.9999)]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = get_sentiment_score(score_text[38], short_brands)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0=brand,1=scores || 0=neg,1=neu,2=pos || 0=neg/pos, 1=score\n",
    "\n",
    "test[1][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = get_sentiment_score(sent_tok[1], short_brands)\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_array = np.array([sent_text_lower[21]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sent_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTENCE get sentiment if brand present in text\n",
    "\n",
    "def get_sentiment_sentence(sent_tok, brands):\n",
    "    \n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    scores = []\n",
    "    \n",
    "    for brand in brands:\n",
    "        for sent in sent_tok:\n",
    "            if brand in sent:\n",
    "                score = list(dict.items(analyser.polarity_scores(sent)))\n",
    "                scores.append([brand, score])\n",
    "          \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Allstate', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Allstate',\n",
       "  [('neg', 0.167), ('neu', 0.833), ('pos', 0.0), ('compound', -0.1027)]],\n",
       " ['Medicare',\n",
       "  [('neg', 0.0), ('neu', 0.543), ('pos', 0.457), ('compound', 0.6369)]],\n",
       " ['Medicare',\n",
       "  [('neg', 0.0), ('neu', 0.787), ('pos', 0.213), ('compound', 0.6597)]],\n",
       " ['Medicare',\n",
       "  [('neg', 0.101), ('neu', 0.504), ('pos', 0.396), ('compound', 0.6249)]]]"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = get_sentiment_sentence(low[5], short_brands)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to create col of sent tokens in df\n",
    "\n",
    "slic = temp_text[0:10]\n",
    "low = sent_for_spacy(slic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accenture', 'adidas', 'adobe', 'agricultural bank of china', 'alibaba', 'amazon', 'american express', 'apple', 'at&t', 'baidu', 'bank of america', 'bank of china', 'bmw', 'budweiser', 'chase', 'china construction bank', 'china life', 'china mobile', 'cisco', 'citi', 'coca-cola', 'colgate', 'commonwealth bank of australia', 'costco', 'deutsche telekom', 'dhl', 'disney', 'ebay', 'exxonmobil', 'facebook', 'fedex', 'ford', 'gillette', 'google', 'gucci', 'hdfc bank', 'hermès', 'honda', 'hp', 'hsbc', 'huawei', 'ibm', 'icbc', 'ikea', 'instagram', 'intel', 'jp morgan', 'jd.com', 'kfc', \"l'oréal paris\", 'linkedin', 'louis vuitton', \"lowe's\", 'marlboro', 'mastercard', \"mcdonald's\", 'mercedes benz', 'microsoft', 'moutai', 'movistar', 'netflix', 'nike', 'oracle', 'pampers', 'paypal', 'pepsi', 'salesforce', 'samsung', 'shell', 'siemens', 'spectrum', 'starbucks', 'subway', 'tencent', 'the home depot', 'toyota', 'uber', 'us bank', 'verizon', 'visa', 'vodafone', 'walmart', 'wells fargo', 'xfinity', 'youtube', 'zara, 'tide', 'whole foods', 'secret service', 'allstate', 'twitter', 'nintendo', 'lincoln', 'medicare', 'philadelphia eagles', 'super bowl', 'aarp', 'nfl', 'miller', 'farmers', 'geico', 'santa', 'cardi b', 'enbrel', 'fbi', 'disney', 'christmas', 'navy', 'humana', 'discover', 'prudential', 'tresiba', 'north american trade agreement', 'dodge', 'at&t', 'ebay', 'trump', 'obama', 'cbs', 'bridgestone', 'hyundai', 'kia', 'mexico', 'chipotle', 'directv', 'social media', 'kanye', 'tmz', 'applebee', 'stanford', 'target', 'alexa', 'kavanaugh', 'autozone', 'centrum', 'chocolate', 'vaseline', 'chp', 'aclu', 'ford', 'army', 'the big bang theory', 'nasa', 'fox', 'kfc', 'ariana grande', 'mcdonald', 'chevy', 'walmart', 'downy', 'nyquil', 'ancestrydna', 'pillsbury', 'amazon', 'sprint', 'iphone', 'bmx', 'liberty mutual', 'target', 'universal studios', 'microsoft']\n",
      "['Accenture', 'Adidas', 'Adobe', 'Agricultural bank of china', 'Alibaba', 'Amazon', 'American express', 'Apple', 'At&t', 'Baidu', 'Bank of america', 'Bank of china', 'Bmw', 'Budweiser', 'Chase', 'China construction bank', 'China life', 'China mobile', 'Cisco', 'Citi', 'Cocacola', 'Colgate', 'Commonwealth bank of australia', 'Costco', 'Deutsche telekom', 'Dhl', 'Disney', 'Ebay', 'Exxonmobil', 'Facebook', 'Fedex', 'Ford', 'Gillette', 'Google', 'Gucci', 'Hdfc bank', 'Hermès', 'Honda', 'Hp', 'Hsbc', 'Huawei', 'Ibm', 'Icbc', 'Ikea', 'Instagram', 'Intel', 'Jp morgan', 'Jd.com', 'Kfc', 'Loréal paris', 'Linkedin', 'Louis vuitton', 'Lowes', 'Marlboro', 'Mastercard', 'Mcdonalds', 'Mercedes benz', 'Microsoft', 'Moutai', 'Movistar', 'Netflix', 'Nike', 'Oracle', 'Pampers', 'Paypal', 'Pepsi', 'Salesforce', 'Samsung', 'Shell', 'Siemens', 'Spectrum', 'Starbucks', 'Subway', 'Tencent', 'The home depot', 'Toyota', 'Uber', 'Us bank', 'Verizon', 'Visa', 'Vodafone', 'Walmart', 'Wells fargo', 'Xfinity', 'Youtube', 'Zara', 'Tide', 'Whole foods', 'Secret service', 'Allstate', 'Twitter', 'Nintendo', 'Lincoln', 'Medicare', 'Philadelphia eagles', 'Super bowl', 'Aarp', 'Nfl', 'Miller', 'Farmers', 'Geico', 'Santa', 'Cardi b', 'Enbrel', 'Fbi', 'Disney', 'Christmas', 'Navy', 'Humana', 'Discover', 'Prudential', 'Tresiba', 'North american trade agreement', 'Dodge', 'At&t', 'Ebay', 'Trump', 'Obama', 'Cbs', 'Bridgestone', 'Hyundai', 'Kia', 'Mexico', 'Chipotle', 'Directv', 'Social media', 'Kanye', 'Tmz', 'Applebee', 'Stanford', 'Target', 'Alexa', 'Kavanaugh', 'Autozone', 'Centrum', 'Chocolate', 'Vaseline', 'Chp', 'Aclu', 'Ford', 'Army', 'The big bang theory', 'Nasa', 'Fox', 'Kfc', 'Ariana grande', 'Mcdonald', 'Chevy', 'Walmart', 'Downy', 'Nyquil', 'Ancestrydna', 'Pillsbury', 'Amazon', 'Sprint', 'Iphone', 'Bmx', 'Liberty mutual', 'Target', 'Universal studios', 'Microsoft']\n"
     ]
    }
   ],
   "source": [
    "with open(\"brand_list.txt\") as f:\n",
    "    short_brands = f.read().replace('\\n', '').lower()\n",
    "print(short_brands)\n",
    "y = short_brands\n",
    "y = list(y.replace(\"'\", '').replace('[', '').replace(']', '').replace('\\\"','').replace('-', '').split(', '))\n",
    "short_brands = [x.capitalize() for x in y]\n",
    "print(short_brands)\n",
    "with open ('data/short_brands.pkl', 'wb') as f:\n",
    "    pickle.dump(short_brands, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
