{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import datetime\n",
    "import string\n",
    "from string import digits\n",
    "import collections\n",
    "import scipy.stats as scs\n",
    "import cc_pipeline as P\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "#sentiment and language\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import vaderSentiment\n",
    "from langdetect import detect\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from spacy import displacy\n",
    "\n",
    "#machine learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import chi2\n",
    "import knee_locator\n",
    "\n",
    "#plotting\n",
    "from bokeh.plotting import figure, show, output_file, output_notebook, ColumnDataSource\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "import umap\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inline_text(show_raw):\n",
    "    \n",
    "    '''returns show text without timestamps'''\n",
    "    \n",
    "    temp = \" \".join( [\"\\n\".join( x.split(\"\\n\")[2:] ) for x in show_raw.split(\"\\n\\n\")] )\n",
    "    temp = temp.split('\\n')\n",
    "    temp = \" \".join(temp)\n",
    "    return temp\n",
    "\n",
    "def clean_for_spacy(text_list):\n",
    "    \n",
    "    '''cleans all text and creates new column in dataframe'''\n",
    "    \n",
    "    doc_list = []\n",
    "    for doc in text_list:\n",
    "        doc_list.append(inline_text(doc))\n",
    "    return doc_list\n",
    "\n",
    "def clean_for_spacy_lower(text_list):\n",
    "    \n",
    "    '''cleans all text and creates new column in dataframe'''\n",
    "    \n",
    "    doc_list = []\n",
    "    for doc in text_list:\n",
    "        doc_list.append(inline_text(doc).lower())\n",
    "    return doc_list\n",
    "\n",
    "def sent_for_spacy(text_list):\n",
    "    \n",
    "    '''cleans all text and creates new column in dataframe'''\n",
    "    \n",
    "    doc_list = []\n",
    "    for doc in text_list:\n",
    "        cleaned = inline_text(doc)\n",
    "        tok = sent_tokenize(cleaned)\n",
    "        doc_list.append(tok)\n",
    "    return doc_list\n",
    "\n",
    "def lang_detect(doc_series):\n",
    "    \n",
    "    lang = []\n",
    "    for x in doc_series:\n",
    "        eng = 'en'\n",
    "        span = 'es'\n",
    "\n",
    "        try:\n",
    "            if detect(x) == eng:\n",
    "                lang.append(eng)\n",
    "            else:\n",
    "                lang.append(span)\n",
    "        except:\n",
    "            lang.append(None)\n",
    "            \n",
    "    return lang\n",
    "\n",
    "def get_orgs(chunks):\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    orgs = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        document = nlp(chunk)\n",
    "        labels = set([w.label_ for w in document.ents]) \n",
    "\n",
    "        for label in labels: \n",
    "\n",
    "            temp_entities = [e for e in document.ents if label==e.label_] \n",
    "            temp_entities = list(set(temp_entities)) \n",
    "\n",
    "            if label == 'ORG':\n",
    "                orgs.append(str(temp_entities))\n",
    "                \n",
    "    orgs = \" \".join(orgs)\n",
    "    return orgs\n",
    "\n",
    "def clean_text(doc):\n",
    "    '''cleans and lemmatizes a string by removing punc, characters, digits, and len(words) < 3'''\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    punct = ('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~♪¿’')\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = []\n",
    "    \n",
    "    doc = doc.split('\\n')\n",
    "    doc = ' '.join(doc)\n",
    "    doc = doc.split('-')\n",
    "    doc = ' '.join(doc)\n",
    "    doc = doc.split('...')\n",
    "    doc = ' '.join(doc)\n",
    "    doc = word_tokenize(doc)\n",
    "\n",
    "    a = [char for char in doc if char not in punct]\n",
    "    b = [w for w in a if w not in stop_words] \n",
    "    c = [w for w in b if len(w) > 3]\n",
    "    d = [x for x in c if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n",
    "\n",
    "    e = ' '.join(d)\n",
    "    f = e.lower()\n",
    "    g = f.translate(remove_digits)\n",
    "    cleaned = str(g)\n",
    "    doc = word_tokenize(cleaned)\n",
    "    \n",
    "    for val in doc:\n",
    "        doc_temp = wordnet_lemmatizer.lemmatize(val)\n",
    "        lemmatized.append(doc_temp)\n",
    "    doc = ' '.join(lemmatized)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def clean_and_return(docs_list):\n",
    "    \n",
    "    docs = []\n",
    "    for cc in docs_list:\n",
    "        cleaned_temp = clean_text(cc)\n",
    "        docs.append(cleaned_temp)\n",
    "        \n",
    "    return docs\n",
    "\n",
    "def get_sentiment_score(doc, brands):\n",
    "    \n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    temp_doc = doc.split()\n",
    "    temp = (brand for brand in brands if brand in temp_doc)\n",
    "    all_brands = []\n",
    "    scores = []\n",
    "    if any(temp):\n",
    "        for brand in temp:\n",
    "            all_brands.append(brand)\n",
    "            score = list(dict.items(analyser.polarity_scores(doc)))\n",
    "            scores.append(score)\n",
    "    return (all_brands, scores)\n",
    "\n",
    "def get_sentiment_sentence(sent_tok, brands):\n",
    "    \n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    scores = []\n",
    "    \n",
    "    for brand in brands:\n",
    "        for sent in sent_tok:\n",
    "            if brand in sent:\n",
    "                score = list(dict.items(analyser.polarity_scores(sent)))\n",
    "                scores.append([brand, score])\n",
    "          \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = pd.read_csv('data/cc_1000_text.csv', encoding='utf=8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english only (for testing)\n",
    "\n",
    "doc_series = pd.Series(sent_df['text'].values)\n",
    "language = lang_detect(doc_series)\n",
    "sent_df['language'] = language\n",
    "english = sent_df[sent_df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brand_text = clean_and_return(lines)\n",
    "# removetable = str.maketrans('', '', \"/.\")\n",
    "# brand_text = [s.translate(removetable) for s in brand_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_text = english['text'].values\n",
    "sent_text = clean_for_spacy(temp_text)\n",
    "sent_text_lower = clean_for_spacy_lower(temp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tok = sent_tokenize(sent_text_lower[1])\n",
    "type(sent_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_text_lower[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_text = clean_and_return(sent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = \" \".join(sent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #break into chunks for spaCy\n",
    "\n",
    "# n = 100000\n",
    "# chunks = [corpus[i:i+n] for i in range(0, len(corpus), n)]\n",
    "# len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# org_list = get_orgs(chunks)\n",
    "# org_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brands = pd.read_csv('data/all_brands.csv', encoding='utf-8', header=None)\n",
    "# brand_names = brands[0].values\n",
    "# brand_names = list(brand_names)\n",
    "# lines = (lines + str(brand_names))\n",
    "# lines = lines.lower()\n",
    "\n",
    "# #conver string of list into list\n",
    "# import ast\n",
    "# x = lines\n",
    "# x = list(x.replace(\"'\", '').replace('[', '').replace(']', '').split(', '))\n",
    "# lines = x\n",
    "\n",
    "# # with open ('data/brands.pkl', 'wb') as f:\n",
    "# #     pickle.dump(lines, f)\n",
    "    \n",
    "# # with open ('data/brands.pkl', 'rb') as r:\n",
    "# #     brands = pickle.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = short_brands\n",
    "# y = list(y.replace(\"'\", '').replace('[', '').replace(']', '').replace('\\\"','').replace('-', '').split(', '))\n",
    "# short_brands = y\n",
    "# with open ('data/short_brands_lower.pkl', 'wb') as f:\n",
    "#     pickle.dump(short_brands, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sentiment if brand present in text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['santa', 'christmas'],\n",
       " [[('neg', 0.048), ('neu', 0.622), ('pos', 0.331), ('compound', 0.9998)],\n",
       "  [('neg', 0.048), ('neu', 0.622), ('pos', 0.331), ('compound', 0.9998)]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = get_sentiment_score(score_text[1], short_brands)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['netflix', 'santa', 'trump', 'obama', 'chocolate', 'army'],\n",
       " [[('neg', 0.085), ('neu', 0.673), ('pos', 0.241), ('compound', 0.9999)],\n",
       "  [('neg', 0.085), ('neu', 0.673), ('pos', 0.241), ('compound', 0.9999)],\n",
       "  [('neg', 0.085), ('neu', 0.673), ('pos', 0.241), ('compound', 0.9999)],\n",
       "  [('neg', 0.085), ('neu', 0.673), ('pos', 0.241), ('compound', 0.9999)],\n",
       "  [('neg', 0.085), ('neu', 0.673), ('pos', 0.241), ('compound', 0.9999)],\n",
       "  [('neg', 0.085), ('neu', 0.673), ('pos', 0.241), ('compound', 0.9999)]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = get_sentiment_score(score_text[38], short_brands)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0=brand,1=scores || 0=neg,1=neu,2=pos || 0=neg/pos, 1=score\n",
    "\n",
    "test[1][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = get_sentiment_score(sent_tok[3], short_brands)\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_array = np.array([sent_text_lower[21]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Allstate', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Allstate',\n",
       "  [('neg', 0.167), ('neu', 0.833), ('pos', 0.0), ('compound', -0.1027)]],\n",
       " ['Medicare',\n",
       "  [('neg', 0.0), ('neu', 0.543), ('pos', 0.457), ('compound', 0.6369)]],\n",
       " ['Medicare',\n",
       "  [('neg', 0.0), ('neu', 0.787), ('pos', 0.213), ('compound', 0.6597)]],\n",
       " ['Medicare',\n",
       "  [('neg', 0.101), ('neu', 0.504), ('pos', 0.396), ('compound', 0.6249)]]]"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = get_sentiment_sentence(low[5], short_brands)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to create col of sent tokens in df\n",
    "\n",
    "slic = temp_text[0:10]\n",
    "low = sent_for_spacy(slic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Accenture', 'Adidas', 'Adobe', 'Agricultural bank of china', 'Alibaba', 'Amazon', 'American express', 'Apple', 'At&t', 'Baidu', 'Bank of america', 'Bank of china', 'Bmw', 'Budweiser', 'Chase', 'China construction bank', 'China life', 'China mobile', 'Cisco', 'Citi', 'Cocacola', 'Colgate', 'Commonwealth bank of australia', 'Costco', 'Deutsche telekom', 'Dhl', 'Disney', 'Ebay', 'Exxonmobil', 'Facebook', 'Fedex', 'Ford', 'Gillette', 'Google', 'Gucci', 'Hdfc bank', 'Hermès', 'Honda', 'Hp', 'Hsbc', 'Huawei', 'Ibm', 'Icbc', 'Ikea', 'Instagram', 'Intel', 'Jp morgan', 'Jd.com', 'Kfc', 'Loréal paris', 'Linkedin', 'Louis vuitton', 'Lowes', 'Marlboro', 'Mastercard', 'Mcdonalds', 'Mercedes benz', 'Microsoft', 'Moutai', 'Movistar', 'Netflix', 'Nike', 'Oracle', 'Pampers', 'Paypal', 'Pepsi', 'Salesforce', 'Samsung', 'Shell', 'Siemens', 'Spectrum', 'Starbucks', 'Subway', 'Tencent', 'The home depot', 'Toyota', 'Uber', 'Us bank', 'Verizon', 'Visa', 'Vodafone', 'Walmart', 'Wells fargo', 'Xfinity', 'Youtube', 'Zara', 'Tide', 'Whole foods', 'Secret service', 'Allstate', 'Twitter', 'Nintendo', 'Lincoln', 'Medicare', 'Philadelphia eagles', 'Super bowl', 'Aarp', 'Nfl', 'Miller', 'Farmers', 'Geico', 'Santa', 'Cardi b', 'Enbrel', 'Fbi', 'Disney', 'Christmas', 'Navy', 'Humana', 'Discover', 'Prudential', 'Tresiba', 'North american trade agreement', 'Dodge', 'At&t', 'Ebay', 'Trump', 'Obama', 'Cbs', 'Bridgestone', 'Hyundai', 'Kia', 'Mexico', 'Chipotle', 'Directv', 'Social media', 'Kanye', 'Tmz', 'Applebee', 'Stanford', 'Target', 'Alexa', 'Kavanaugh', 'Autozone', 'Centrum', 'Chocolate', 'Vaseline', 'Chp', 'Aclu', 'Ford', 'Army', 'The big bang theory', 'Nasa', 'Fox', 'Kfc', 'Ariana grande', 'Mcdonald', 'Chevy', 'Walmart', 'Downy', 'Nyquil', 'Ancestrydna', 'Pillsbury', 'Amazon', 'Sprint', 'Iphone', 'Bmx', 'Liberty mutual', 'Target', 'Universal studios', 'Microsoft']\n"
     ]
    }
   ],
   "source": [
    "with open(\"brand_list.txt\") as f:\n",
    "    short_brands = f.read().replace('\\n', '').lower()\n",
    "    \n",
    "y = short_brands\n",
    "#use replace instead of str.maketrans for special characters\n",
    "y = list(y.replace(\"'\", '').replace('[', '').replace(']', '').replace('\\\"','').replace('-', '').split(', '))\n",
    "short_brands = [x.capitalize() for x in y]\n",
    "print(short_brands)\n",
    "\n",
    "with open ('data/short_brands.pkl', 'wb') as f:\n",
    "    pickle.dump(short_brands, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('data/sentiments.pkl', 'rb') as r:\n",
    "     sentiments = pickle.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Farmers', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Farmers', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Allstate', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Allstate',\n",
       "  [('neg', 0.167), ('neu', 0.833), ('pos', 0.0), ('compound', -0.1027)]],\n",
       " ['Netflix', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Xfinity', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Xfinity',\n",
       "  [('neg', 0.0), ('neu', 0.882), ('pos', 0.118), ('compound', 0.4939)]],\n",
       " ['Nintendo',\n",
       "  [('neg', 0.103), ('neu', 0.653), ('pos', 0.243), ('compound', 0.497)]],\n",
       " ['Nintendo', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Allstate', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Allstate',\n",
       "  [('neg', 0.167), ('neu', 0.833), ('pos', 0.0), ('compound', -0.1027)]],\n",
       " ['Medicare',\n",
       "  [('neg', 0.0), ('neu', 0.543), ('pos', 0.457), ('compound', 0.6369)]],\n",
       " ['Medicare',\n",
       "  [('neg', 0.0), ('neu', 0.787), ('pos', 0.213), ('compound', 0.6597)]],\n",
       " ['Medicare',\n",
       "  [('neg', 0.101), ('neu', 0.504), ('pos', 0.396), ('compound', 0.6249)]],\n",
       " ['Lincoln',\n",
       "  [('neg', 0.0), ('neu', 0.754), ('pos', 0.246), ('compound', 0.4466)]],\n",
       " ['Lincoln',\n",
       "  [('neg', 0.0), ('neu', 0.432), ('pos', 0.568), ('compound', 0.3885)]],\n",
       " ['Lincoln',\n",
       "  [('neg', 0.0), ('neu', 0.432), ('pos', 0.568), ('compound', 0.3885)]],\n",
       " ['Lincoln', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Lincoln', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Lincoln', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Lincoln',\n",
       "  [('neg', 0.0), ('neu', 0.452), ('pos', 0.548), ('compound', 0.5622)]],\n",
       " ['Lincoln',\n",
       "  [('neg', 0.0), ('neu', 0.452), ('pos', 0.548), ('compound', 0.5622)]],\n",
       " ['Lincoln',\n",
       "  [('neg', 0.0), ('neu', 0.855), ('pos', 0.145), ('compound', 0.4995)]],\n",
       " ['Lincoln', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Lincoln', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Lincoln', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Lincoln', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " ['Lincoln',\n",
       "  [('neg', 0.216), ('neu', 0.784), ('pos', 0.0), ('compound', -0.6289)]],\n",
       " ['Lincoln',\n",
       "  [('neg', 0.184), ('neu', 0.69), ('pos', 0.126), ('compound', -0.5574)]],\n",
       " ['Lincoln',\n",
       "  [('neg', 0.0), ('neu', 0.505), ('pos', 0.495), ('compound', 0.7824)]]]"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Farmers', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]],\n",
       " 0.0)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most polar brands\n",
    "negative = (sentiments[0], sentiments[0][1][0][1])\n",
    "negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.Series(sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neg', 0.831), ('neu', 0.169), ('pos', 0.0), ('compound', -0.5994)]\n"
     ]
    }
   ],
   "source": [
    "def max_value(sentiments):\n",
    "    return max([sublist[-1] for sublist in sentiments])\n",
    "\n",
    "print(max_value(sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zara', [('neg', 0.0), ('neu', 1.0), ('pos', 0.0), ('compound', 0.0)]]"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_neg = max(test)\n",
    "most_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'tuple' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-439-28cf14accde7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mneg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'tuple' and 'str'"
     ]
    }
   ],
   "source": [
    "neg = max(flat_test, key=lambda x: x[1])\n",
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-434-dbc0dfd536e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-434-dbc0dfd536e8>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pos = max(test, key=lambda x: x[3])\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Farmers</td>\n",
       "      <td>[(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Farmers</td>\n",
       "      <td>[(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allstate</td>\n",
       "      <td>[(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Allstate</td>\n",
       "      <td>[(neg, 0.167), (neu, 0.833), (pos, 0.0), (comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netflix</td>\n",
       "      <td>[(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Xfinity</td>\n",
       "      <td>[(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Xfinity</td>\n",
       "      <td>[(neg, 0.0), (neu, 0.882), (pos, 0.118), (comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nintendo</td>\n",
       "      <td>[(neg, 0.103), (neu, 0.653), (pos, 0.243), (co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nintendo</td>\n",
       "      <td>[(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Allstate</td>\n",
       "      <td>[(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Allstate</td>\n",
       "      <td>[(neg, 0.167), (neu, 0.833), (pos, 0.0), (comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Medicare</td>\n",
       "      <td>[(neg, 0.0), (neu, 0.543), (pos, 0.457), (comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Medicare</td>\n",
       "      <td>[(neg, 0.0), (neu, 0.787), (pos, 0.213), (comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Medicare</td>\n",
       "      <td>[(neg, 0.101), (neu, 0.504), (pos, 0.396), (co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.0), (neu, 0.754), (pos, 0.246), (comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.0), (neu, 0.432), (pos, 0.568), (comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.0), (neu, 0.432), (pos, 0.568), (comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.0), (neu, 0.452), (pos, 0.548), (comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.0), (neu, 0.452), (pos, 0.548), (comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.0), (neu, 0.855), (pos, 0.145), (comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.216), (neu, 0.784), (pos, 0.0), (comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.184), (neu, 0.69), (pos, 0.126), (com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>[(neg, 0.0), (neu, 0.505), (pos, 0.495), (comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0                                                  1\n",
       "0    Farmers  [(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...\n",
       "1    Farmers  [(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...\n",
       "2   Allstate  [(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...\n",
       "3   Allstate  [(neg, 0.167), (neu, 0.833), (pos, 0.0), (comp...\n",
       "4    Netflix  [(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...\n",
       "5    Xfinity  [(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...\n",
       "6    Xfinity  [(neg, 0.0), (neu, 0.882), (pos, 0.118), (comp...\n",
       "7   Nintendo  [(neg, 0.103), (neu, 0.653), (pos, 0.243), (co...\n",
       "8   Nintendo  [(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...\n",
       "9   Allstate  [(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...\n",
       "10  Allstate  [(neg, 0.167), (neu, 0.833), (pos, 0.0), (comp...\n",
       "11  Medicare  [(neg, 0.0), (neu, 0.543), (pos, 0.457), (comp...\n",
       "12  Medicare  [(neg, 0.0), (neu, 0.787), (pos, 0.213), (comp...\n",
       "13  Medicare  [(neg, 0.101), (neu, 0.504), (pos, 0.396), (co...\n",
       "14   Lincoln  [(neg, 0.0), (neu, 0.754), (pos, 0.246), (comp...\n",
       "15   Lincoln  [(neg, 0.0), (neu, 0.432), (pos, 0.568), (comp...\n",
       "16   Lincoln  [(neg, 0.0), (neu, 0.432), (pos, 0.568), (comp...\n",
       "17   Lincoln  [(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...\n",
       "18   Lincoln  [(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...\n",
       "19   Lincoln  [(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...\n",
       "20   Lincoln  [(neg, 0.0), (neu, 0.452), (pos, 0.548), (comp...\n",
       "21   Lincoln  [(neg, 0.0), (neu, 0.452), (pos, 0.548), (comp...\n",
       "22   Lincoln  [(neg, 0.0), (neu, 0.855), (pos, 0.145), (comp...\n",
       "23   Lincoln  [(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...\n",
       "24   Lincoln  [(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...\n",
       "25   Lincoln  [(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...\n",
       "26   Lincoln  [(neg, 0.0), (neu, 1.0), (pos, 0.0), (compound...\n",
       "27   Lincoln  [(neg, 0.216), (neu, 0.784), (pos, 0.0), (comp...\n",
       "28   Lincoln  [(neg, 0.184), (neu, 0.69), (pos, 0.126), (com...\n",
       "29   Lincoln  [(neg, 0.0), (neu, 0.505), (pos, 0.495), (comp..."
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(sentiments, )\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-459-f5dd7548d957>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-459-f5dd7548d957>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    df[[,]] = pd.DataFrame(df[1].values.tolist(), index=df.index)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df[[,]] = pd.DataFrame(df[1].values.tolist(), index=df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_array = [(sent[0], sent[1][0][1], sent[1][2][1]) for sent in sentiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = pd.DataFrame(sentiment_array, columns=['label', 'neg','pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23714</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39181</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16487</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6792</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45957</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45948</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23710</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6795</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39185</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58409</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58440</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16490</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76274</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73676</th>\n",
       "      <td>Chipotle</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20005</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17633</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25635</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10164</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35476</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33104</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68782</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14306</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29777</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68970</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40133</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21316</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63035</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32911</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66408</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66764</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63036</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42334</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55395</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66390</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66994</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83022</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36787</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82755</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13193</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4507</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66853</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81856</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66757</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17440</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28664</th>\n",
       "      <td>Christmas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35497</th>\n",
       "      <td>Spectrum</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7095</th>\n",
       "      <td>Spectrum</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20026</th>\n",
       "      <td>Spectrum</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label  neg    pos\n",
       "23714     Amazon  0.0  1.000\n",
       "39181     Amazon  0.0  1.000\n",
       "16487     Amazon  0.0  1.000\n",
       "6792      Amazon  0.0  1.000\n",
       "45957     Amazon  0.0  1.000\n",
       "31958     Amazon  0.0  1.000\n",
       "45948     Amazon  0.0  1.000\n",
       "23710     Amazon  0.0  1.000\n",
       "6795      Amazon  0.0  1.000\n",
       "39185     Amazon  0.0  1.000\n",
       "58409     Amazon  0.0  1.000\n",
       "58440     Amazon  0.0  1.000\n",
       "16490     Amazon  0.0  1.000\n",
       "31961     Amazon  0.0  1.000\n",
       "76274    Lincoln  0.0  0.855\n",
       "73676   Chipotle  0.0  0.808\n",
       "20005   Facebook  0.0  0.793\n",
       "17633   Facebook  0.0  0.793\n",
       "25635   Facebook  0.0  0.793\n",
       "10164   Facebook  0.0  0.793\n",
       "35476   Facebook  0.0  0.793\n",
       "33104   Facebook  0.0  0.793\n",
       "68782  Christmas  0.0  0.791\n",
       "14306  Christmas  0.0  0.791\n",
       "29777  Christmas  0.0  0.791\n",
       "68970  Christmas  0.0  0.791\n",
       "40133  Christmas  0.0  0.778\n",
       "21316  Christmas  0.0  0.778\n",
       "63035  Christmas  0.0  0.778\n",
       "32911  Christmas  0.0  0.778\n",
       "66408  Christmas  0.0  0.778\n",
       "66764  Christmas  0.0  0.778\n",
       "63036  Christmas  0.0  0.778\n",
       "42334  Christmas  0.0  0.778\n",
       "55395  Christmas  0.0  0.778\n",
       "66390  Christmas  0.0  0.778\n",
       "66994  Christmas  0.0  0.778\n",
       "83022  Christmas  0.0  0.778\n",
       "36787  Christmas  0.0  0.778\n",
       "82755  Christmas  0.0  0.778\n",
       "13193  Christmas  0.0  0.778\n",
       "4507   Christmas  0.0  0.778\n",
       "66853  Christmas  0.0  0.778\n",
       "81856  Christmas  0.0  0.778\n",
       "66757  Christmas  0.0  0.778\n",
       "17440  Christmas  0.0  0.778\n",
       "28664  Christmas  0.0  0.778\n",
       "35497   Spectrum  0.0  0.767\n",
       "7095    Spectrum  0.0  0.767\n",
       "20026   Spectrum  0.0  0.767"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = sent_df.sort_values('pos', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74247</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33392</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32382</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19973</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35444</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39907</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17412</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32883</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16448</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34818</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19347</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16911</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31919</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40416</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17921</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76773</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76769</th>\n",
       "      <td>Twitter</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31882</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39872</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31884</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19365</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19312</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33357</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19367</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33921</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17377</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16929</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19310</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40434</th>\n",
       "      <td>Kia</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31648</th>\n",
       "      <td>Army</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31647</th>\n",
       "      <td>Target</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31646</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31645</th>\n",
       "      <td>Shell</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31644</th>\n",
       "      <td>Shell</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31643</th>\n",
       "      <td>Shell</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31642</th>\n",
       "      <td>Shell</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31641</th>\n",
       "      <td>Shell</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31640</th>\n",
       "      <td>Shell</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31639</th>\n",
       "      <td>Shell</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31638</th>\n",
       "      <td>Shell</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31653</th>\n",
       "      <td>Centrum</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31654</th>\n",
       "      <td>Centrum</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31656</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31668</th>\n",
       "      <td>Allstate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31675</th>\n",
       "      <td>Target</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31673</th>\n",
       "      <td>Allstate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31672</th>\n",
       "      <td>Sprint</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31671</th>\n",
       "      <td>Facebook</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31670</th>\n",
       "      <td>Allstate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31669</th>\n",
       "      <td>Army</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31667</th>\n",
       "      <td>Allstate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31657</th>\n",
       "      <td>Allstate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31665</th>\n",
       "      <td>Netflix</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31664</th>\n",
       "      <td>Allstate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31663</th>\n",
       "      <td>Discover</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31661</th>\n",
       "      <td>Discover</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31660</th>\n",
       "      <td>Downy</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31658</th>\n",
       "      <td>Downy</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86684</th>\n",
       "      <td>Chevy</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86685 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           label    neg    pos\n",
       "74247    Lincoln  0.831  0.000\n",
       "33392        Kia  0.808  0.000\n",
       "32382        Kia  0.808  0.000\n",
       "19973        Kia  0.808  0.000\n",
       "35444        Kia  0.808  0.000\n",
       "39907        Kia  0.808  0.000\n",
       "17412        Kia  0.808  0.000\n",
       "32883        Kia  0.808  0.000\n",
       "16448        Kia  0.808  0.000\n",
       "34818        Kia  0.808  0.000\n",
       "19347        Kia  0.808  0.000\n",
       "16911        Kia  0.808  0.000\n",
       "31919        Kia  0.808  0.000\n",
       "40416        Kia  0.808  0.000\n",
       "17921        Kia  0.808  0.000\n",
       "76773    Twitter  0.778  0.000\n",
       "76769    Twitter  0.778  0.000\n",
       "31882        Kia  0.759  0.000\n",
       "39872        Kia  0.759  0.000\n",
       "19991        Kia  0.759  0.000\n",
       "31884        Kia  0.759  0.000\n",
       "19365        Kia  0.759  0.000\n",
       "19312        Kia  0.759  0.000\n",
       "33357        Kia  0.759  0.000\n",
       "19367        Kia  0.759  0.000\n",
       "33921        Kia  0.759  0.000\n",
       "17377        Kia  0.759  0.000\n",
       "16929        Kia  0.759  0.000\n",
       "19310        Kia  0.759  0.000\n",
       "40434        Kia  0.759  0.000\n",
       "...          ...    ...    ...\n",
       "31648       Army  0.000  0.000\n",
       "31647     Target  0.000  0.000\n",
       "31646  Microsoft  0.000  0.224\n",
       "31645      Shell  0.000  0.000\n",
       "31644      Shell  0.000  0.000\n",
       "31643      Shell  0.000  0.000\n",
       "31642      Shell  0.000  0.000\n",
       "31641      Shell  0.000  0.000\n",
       "31640      Shell  0.000  0.000\n",
       "31639      Shell  0.000  0.000\n",
       "31638      Shell  0.000  0.000\n",
       "31653    Centrum  0.000  0.000\n",
       "31654    Centrum  0.000  0.000\n",
       "31656    Lincoln  0.000  0.095\n",
       "31668   Allstate  0.000  0.312\n",
       "31675     Target  0.000  0.000\n",
       "31673   Allstate  0.000  0.298\n",
       "31672     Sprint  0.000  0.202\n",
       "31671   Facebook  0.000  0.235\n",
       "31670   Allstate  0.000  0.298\n",
       "31669       Army  0.000  0.000\n",
       "31667   Allstate  0.000  0.000\n",
       "31657   Allstate  0.000  0.298\n",
       "31665    Netflix  0.000  0.000\n",
       "31664   Allstate  0.000  0.298\n",
       "31663   Discover  0.000  0.000\n",
       "31661   Discover  0.000  0.000\n",
       "31660      Downy  0.000  0.000\n",
       "31658      Downy  0.000  0.000\n",
       "86684      Chevy  0.000  0.171\n",
       "\n",
       "[86685 rows x 3 columns]"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "negative = sent_df.sort_values('neg', ascending=False)\n",
    "negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Lincoln', 'Kia', 'Twitter', 'Kanye', 'Apple', 'Kavanaugh', 'Ford',\n",
       "       'Army', 'Intel', 'Fox', 'Social media', 'Christmas', 'Lowes',\n",
       "       'Facebook', 'Shell', 'Oracle', 'Adobe', 'Medicare', 'Disney',\n",
       "       'Spectrum', 'Miller', 'Starbucks', 'Nike', 'Trump', 'Toyota',\n",
       "       'Navy', 'Santa', 'Uber', 'Discover', 'Amazon', 'Xfinity', 'Dodge',\n",
       "       'Citi', 'Costco', 'Instagram', 'Chase', 'Allstate', 'Chocolate',\n",
       "       'Enbrel', 'Netflix', 'Google', 'Mexico', 'Sprint', 'Tresiba',\n",
       "       'Chevy', 'Microsoft', 'Alexa', 'Liberty mutual', 'Pampers',\n",
       "       'Applebee', 'Subway', 'Tide', 'Honda', 'Chipotle', 'Marlboro',\n",
       "       'Downy', 'Stanford', 'Pepsi', 'Samsung', 'Ikea', 'Adidas', 'Gucci',\n",
       "       'Hyundai', 'Verizon', 'Obama', 'Walmart', 'Nintendo', 'Visa',\n",
       "       'Nyquil', 'Prudential', 'Nasa', 'Fedex', 'Budweiser', 'Target',\n",
       "       'Gillette', 'Pillsbury', 'Centrum', 'Vaseline', 'Cisco', 'Paypal',\n",
       "       'Youtube', 'Geico', 'Colgate', 'Farmers', 'Zara', 'Iphone',\n",
       "       'Hermès', 'Mastercard', 'Humana'], dtype=object)"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_negative = negative['label'].unique()\n",
    "most_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Amazon', 'Lincoln', 'Chipotle', 'Facebook', 'Christmas',\n",
       "       'Spectrum', 'Chase', 'Shell', 'Medicare', 'Ikea', 'Xfinity',\n",
       "       'Mexico', 'Disney', 'Ford', 'Navy', 'Intel', 'Alexa', 'Centrum',\n",
       "       'Dodge', 'Chevy', 'Citi', 'Santa', 'Trump', 'Google', 'Tide',\n",
       "       'Downy', 'Stanford', 'Fox', 'Pampers', 'Apple', 'Discover',\n",
       "       'Miller', 'Honda', 'Army', 'Chocolate', 'Kia', 'Adidas', 'Target',\n",
       "       'Allstate', 'Nintendo', 'Kanye', 'Prudential', 'Vaseline',\n",
       "       'Starbucks', 'Liberty mutual', 'Budweiser', 'Nasa', 'Hyundai',\n",
       "       'Twitter', 'Zara', 'Kavanaugh', 'Uber', 'Tresiba', 'Instagram',\n",
       "       'Costco', 'Walmart', 'Farmers', 'Toyota', 'Pillsbury', 'Gucci',\n",
       "       'Sprint', 'Visa', 'Verizon', 'Iphone', 'Adobe', 'Samsung',\n",
       "       'Netflix', 'Nike', 'Obama', 'Cisco', 'Marlboro', 'Geico', 'Subway',\n",
       "       'Gillette', 'Youtube', 'Microsoft', 'Colgate', 'Applebee',\n",
       "       'Social media', 'Fedex', 'Pepsi', 'Oracle', 'Lowes', 'Nyquil',\n",
       "       'Paypal', 'Enbrel', 'Humana', 'Hermès', 'Mastercard'], dtype=object)"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_positive = positive['label'].unique()\n",
    "most_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
