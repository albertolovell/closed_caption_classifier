Final Proposal: Closed Caption Unsupervised Learning


Closed caption data allows for an in-depth exploration of the sentiment of a show or broadcast network. Using unstructured closed caption transcripts and metadata such as channel, timestamp, anonymized TV show id, and duration, I plan to explore underlying correlations between the vocabulary and word distribution of a show or broadcast station and the sentiment of those shows using unsupervised learning, clustering, and nlp.

Since the raw data is not well formatted, I plan to create a pipeline to standardize, clean, and transform the entire set. I will then apply modeling techniques to find insteresting insights or relationships inherent in the data.

I anticipate possible issues with the size and formatting of the dataset. Each unique ID contains metadata as well as a url to closed-caption information formatted as a .cc file. I anticipate potential issues during file extraction/conversion and will need to explore the most efficient method to import raw text data for analysis. Also, since this is a large dataset, I may need to implement a cloud-based method during modeling which will take time to set up the environment.

Initially I will tune and evaluate a simple unsupervised learning model to cluster content using various similarity measures. I then plan to move forward by using dimensionality reduction techniques to further discriminate between clusters and create a descriptive interactive visualization.

If possible, I will then use timestamp information to explore any underlying relationships between trending topics and show/channel content, as well as characteristics of word distribution during specific time windows. I would also like to use this data to create a classifier or recommender and explore complex word indentifiers (CWI) for future data as it becomes available.

This project will rely heavily on python's scikitlearn and NLTK libraries. I plan to use visualization libraries such as bokeh, plot.ly, seaborn etc. and any additional proprietary libraries will be cited. I will also acknowledge Gracenote/Nielsen for providing the data in accordance with a signed Non-Disclosure Agreement.


Data ('img/cc_metadata.png', 'img/cc_raw_text.png')

Raw data provided by Gracenote/Nielsen was extracted via api and stored as a .json file with embedded urls linking to .cc closed caption files for each unique TV show ID. 

Metadata includes: channel name, timestamp, title unique id (TV show), duration (milliseconds), url (.cc caption file)
Caption files include: timestamp, duration, captions 

Metadata Example:

 u'_id': {u'$oid': u'5b6dc4b66f40ed0007a35e50'},
 u'channel': u'KCPQ_FOX_SEATTLE',
 u'created_at': {u'$date': u'2018-08-10T17:00:38.690Z'},
 u'duration': 900000,
 u'lang': u'dflt',
 u'machine_id': u'ENSWERCCR7_48_2',
 u'ts': {u'$numberLong': u'1533919500000'},
 u'tui': u'251536137',
 u'tv': {u'$date': u'2018-08-10T16:45:00.000Z'},
 u'url': u'https://s3.amazonaws.com/adm-mpeg-dash/gnvideostream/ENSWERCCR7_48_2/KCPQ_FOX_SEATTLE/ccaptions/2018/08/10/d871a12943c40894b2ef9a244dd5704a.cc',
 u'zip_url': u'https://s3.amazonaws.com/adm-mpeg-dash/gnvideostream/ENSWERCCR7_48_2/KCPQ_FOX_SEATTLE/ccaptions/2018/08/10/d871a12943c40894b2ef9a244dd5704a.cc.gz'
 
Raw Text Example:

1533918614077
634
RENTON CURVES SOUTHBOUND 405
HITTING THE BRAKES CLOSER

1533918614711
634
HITTING THE BRAKES CLOSER
TOWARDS THE SNOHOMISH KING